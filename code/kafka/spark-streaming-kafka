
//spark-shell --master local[4]
import org.apache.spark.streaming.kafka._

 
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3
import  org.apache.spark.storage.StorageLevel

// Create a local StreamingContext with two working thread and batch interval of 1 second.
// The master requires 2 cores to prevent from a starvation scenario.

//val conf = new SparkConf().setMaster("local[2]").setAppName("Kafka-Spark Streaming")
val ssc = new StreamingContext(sc, Seconds(5))
 
val kafkaConf = Map(
  "metadata.broker.list" -> "103.19.88.246:9092",
  "zookeeper.connect" -> "103.19.88.246:2181",
  "group.id" -> "kafka-spark-streaming-example",
  "zookeeper.connection.timeout.ms" -> "1000")
 
val topic = Map("test" -> 1)

val lines = KafkaUtils.createStream(
  ssc,
  kafkaConf,
  topic,
  StorageLevel.MEMORY_ONLY_SER)
  
 val lines = KafkaUtils.createDirectStream[Array[Byte], String, 
  DefaultDecoder, StringDecoder](
  ssc,
  kafkaConf,
  Set(topic))
 
 lines.foreachRDD { rdd =>
  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
  rdd.foreach(println)
} 
  
ssc.start()             // Start the computation
ssc.awaitTermination() 