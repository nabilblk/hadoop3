import kafka.serializer.StringDecoder

import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._
import org.apache.spark.SparkConf

	
	//StreamingExamples.setStreamingLogLevels()
	
	val brokers = "103.19.88.246:2181"
	val topics = "test,test"

     
    // Create context with 2 second batch interval
    val ssc = new StreamingContext(sc, Seconds(30))

    // Create direct kafka stream with brokers and topics
    val topicsSet = topics.split(",").toSet
    val kafkaParams = Map[String, String]("metadata.broker.list" -> brokers)
    val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)
	
	
	//Non-streaming: use this
	 val offsetRanges = Array(
      OffsetRange("sometopic", 0, 110, 220),
      OffsetRange("sometopic", 1, 100, 313),
      OffsetRange("anothertopic", 0, 456, 789)
    )
 
 
	val lines = KafkaUtils.createRDD[String, String, StringDecoder, StringDecoder]( ssc, kafkaParams, offsetRanges)
	
	lines.foreachRDD { rdd =>
		val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
		rdd.foreach(println)
	} 

    // Start the computation
    ssc.start()
    ssc.awaitTermination()